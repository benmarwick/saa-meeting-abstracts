---
title: "The Geometry of Archaeological Thought: ..."
output: bookdown::html_document2
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

# Background

- previous efforts at the history of archaeology (e.g. Bruce Trigger's book)
- previous efforts at text analysis in archaeology (e.g. bibliometrics, citation networks)

# Methods

- source of the data
We collected abstracts from Society of American Archaeology (SAA) annual meeting programs from 1964 to 2020. The total number of abstracts are 68,176 from 40 annual meetings excluding some years without available abstracts. The source of abstract includes papers, posters, workshop posters, symposium represented by paragraphs. The annual programs are available online at https://www.saa.org/annual-meeting/annual-meeting-archives/program-archives. The original format of abstracts on websites includes non-searchable PDF documents coming from images for early years and searchable PDF documents after 2005.

- preparation steps: data cleaning (Cite the quanteda pkg, and things they cite)
For image-based pdfs, we convert them into PNG files, clean up the images using Magick package, and then extract the text using Tesseract package [@Kay2007] for optical character recognition (OCR). We obtain 95-98 percent accuracy of OCR on a word level across years according to the results of error rate for each year. After the text are ready, we use quanteda package for text managing and analyzing [@Benoit2018]. We clean up the text by removing all punctuation and stop words, such as "the", "an", "a", that might have some impacts on our results.

- word embeddings (cite our readings)
Word embeddings is useful tool based machine learning framework for measurement, quantifying, and comparing the semantic relations between words, which enables a wide range of application for temporal changes in word relationships related to the sociocultural domain [@Garg2018; @Kozlowski2019]. There are two major word-level pretrained word embeddings, Weod2Vec and Glove according to different methods [@Rong2014; @Pennington2014]. Since pretrained word embeddings are trained on large corpus such as Google News, they can capture the semantic meaning  of a word very well. 

In this article, we use text2vec package [@Selivanov2016] to train the word embeddings using GloVe model, which calculates the occurrence of a pair of words and creates the co-occurrence matrix. 

- temporal referencing plots (cite our readings)

# Results

- basic details (e.g. word and abstract counts)
- word frequency time series
- TR plots

# Discussion

- ...

# Conclusion

- ...

# Acknowledgements

# References 

<!--- use the bib file --> 



