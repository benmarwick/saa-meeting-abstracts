---
title: "The Geometry of Archaeological Thought: Analysing discplinary change using word embeddings from conference abstracts"
output: bookdown::html_document2
bibliography: references.bib
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

BM: The study of disciplinary change traditionally relies on close reading of a large number of publications, manually sorting them into pre-defined categories and making connections between these groups and external influences [e.g. @trigger1989history]. These methods are time-consuming and expensive (because they depend on full access to a large library of texts), which limits participation in this aspect of archaeology. Computational approaches to analyzing text that can process many publications quickly to generate topics in a bottom-up fashion offer a new way to investigate disciplinary history. This has the advantage of increasing and diversifying participation in study of the history of archaeology, as well as  more comprehensive, systematic, and reproducible analyses of the contents of large number of full-text documents. The methods and data we present here offer empirical basis for what might otherwise be informal claims about the discipline and its history. They are also the type of data that may prompt discussion about the directions the discipline might — or could — take.

BM: In this paper we present a dataset of all available abstracts of papers and posters presented at annual meetings of the Society of American Archaeology. We demonstrate the use of word embeddings, a commonly used computational tool in natural language processing (NLP) and machine learning, as a method to measure, quantify, and compare changes in the way archaeologists write cf. [@Garg2018; @Kozlowski2019]. We present the temporal reference plot as a method for visualizing disciplinary change in archaeology. As a case study, we use these data and methods to explore how archaeologists' writing about explaining and theorizing the past have changed over time.

# Background

BM: Research into the history of archaeology is often organised into two major phases. An early internalist phase, where the story of development of the field is told with limited reference to the influence of political, social, religious, and economic institutions. In the 1980s an external, critical history phase began, seeking to understand the interactions between archaeological practice and its political, economic and social contexts.

BM: Among historians of archaeology there are two major phases in  ‘critical history of archaeology’ in the 1980s had been characterized by the replacement of an 

- previous efforts at the history of archaeology (e.g. Bruce Trigger's book)
- previous efforts at text analysis in archaeology (e.g. bibliometrics, citation networks)
- new phase: we are now in the third phase  - computational text analysis
- one promising new method is WE... successful applications of WE in other fields... 

- background to Word embeddings

LW & BM: Word embeddings are increasingly being used by researchers in novel ways and have become popular in fields such as digital humanities and computational social science. A word embedding is a set of numbers representing a word that generated from a collection of texts. A word embedding characterizes the meaning a word and can be used to reveal latent semantic relationships between pairs of words, often measured by computing the cosine similarity of two embeddings [@antoniak2018evaluating]. The general idea is similar to the concept of Principal Component Analysis (PCA) that reduces a multi-dimensional dataset into a small number of new dimensions that typically capture most of the variation in the data. In this case, a word vector is similar to a principal component because it expresses the meaning of the word into a small set of numeric values (e.g. typically 50-250 values). This set is known as the word embedding or word vector, and we visualize vectors on a plot, the distance between two words estimates the similarity between the two words. One of the most interesting and useful qualities of word vectors is that simple arithmetic operations can reveal semantic relationships between words [@mikolov2013linguistic]. For example if we subtract the word vector values for 'man' from the word vector values for 'king', and add the word vector values for 'woman', the resulting vector will be very close to that of 'queen', such that we can literally compute 'king - man + woman' to get a vector that is approximately the same as 'queen'. 

BM: Word embeddings have been used as evidence in studies of historical change in language and culture [@phillips2017intrinsic; @heuser2017word; @kulkarni2015statistically; @pivovarova2019word]. Semantic relations between words in a corpus are captured by the geometry of the vectors, so vectors can be compared with each other to find a word’s nearest neighbors [@giorcelli2019does].  Vectors can be compared across periods to examine the degree of change a word underwent from one-time interval to the next. For example, @hamilton2016diachronic use word embeddings to study semantic change in English over the last 200 years. They visualise how the word 'gay' shifted from meaning 'cheerful' or 'frolicsome' to referring to homosexuality, and the change in 'broadcast' from referring to 'casting out seeds' to 'transmitting signals', with the rise of television and radio in the early 20th century. @yao2018dynamic generate word embeddings on 99,872 articles from the New York Times, published between January 1990 and July 2016 and their results show major semantic shifts in their words of interest. For example, the word embeddings for the US president names, Obama and Trump, shift from their pre-presidential lives (Obama as a civil rights attorney and university professor; Trump as a real estate developer and TV personality) to the political sphere.

LW:  Word embeddings have also been used to track demographic and occupational social shifts and gender stereotypes. For example, @Garg2018 explored changes in gender stereotypes and attitudes toward ethnic minorities in the 20th and 21st centuries in the US. They created word embeddings from different sources after 1900, including Google News, Google Books, and Corpus of Historical American English using Word2vex algorithm and New York Times, Wikipedia, and Common Crawl using GloVe algorithm. They made custom lists of group words representing gender and ethnicity, and neutral words, such as adjectives and jobs, serving as indicators for stereotypes and attitudes. For example, group words of man consists of he, man, boy, etc., and group words of Asian represented by last names such as li, wong, kim, etc. They compared the association between each group words (gender and ethnic minorities) and neutral words, where the strength of association is calculated by computing average embedding distance. Their results show that jobs such as nurses, dancers, housekeepers are biased toward women and there is an increase in adjectives for describing competence, such as intelligent, after women’s movement in the 1960s-1970s with higher participation rate in jobs. Another bias is the jobs of Asian-American were associated with academic positions reflecting the stereotype of the model minority, and their adjectives shift from negative words describing outsiders before 1950 to stereotype words about personality trait after 1950, with a growth of Asian-American population. This study demonstrates a successful use of word embeddings to detect semantic change in social bias over time with its sociocultural background.

LW: Another example shows how word embeddings are effective for identifying semantic changes in shared understandings of social classes in the 20th century in the US. @Kozlowski analyzed word embeddings generated on millions of books from Google Ngrams texts between 1990 and 2012 using the word2Vec algorithm with the GloVe algorithm as a cross-validation. The results suggest that 'employment', one of the class dimensions, shows most of the semantic changes from titles of formal office, such as 'lords' and 'governor', to 'promoter' and 'mogul` referring to economic status with power. Similarly, the class dimension 'employee' shifts from 'wage' and 'earners' in the early 20th century, 'soldier' during World War I, to 'retirement' and 'qualified' in the end of the century, reflecting the emergence of middle-class. They also found a close relationship between word embeddings in the classes, including education, morality, and cultivation, over time despite there is a major shift in the condition of economy, industry, and employment in the present day. This case study indicates the robustness of word embeddings as a method to examine historical trends by exploring social classes.

# Methods

```{r read-data}
saa_data <- readxl::read_excel(here::here("data", "raw-data", "saa-abstracts-tally.xlsx"))

# basic stats
library(tidyverse)
abstract_num <- sum(saa_data$number_of_abstracts, na.rm = TRUE)
year_num <- length(which(!is.na(saa_data$number_of_abstracts))) # 41 years

# get all txt files of abstracts
abstract_txt_files <- 
  fs::dir_ls(here::here("data/derived-data/"), 
             recurse = TRUE, 
             regexp = '\\.txt$')
all_txts <- tolower(readtext::readtext(abstract_txt_files))

all_word_counts <- sum(stringi::stri_count_words(all_txts))
```

LW: The data for our study comes from the text of abstracts of oral and poster presentations at the annual meetings of the Society of American Archaeology (SAA). The earliest meeting in our data is 1962, and we have some years missing, especially 1995-2003 where PDF files were not available to us. The total number of abstracts is `r prettyNum(abstract_num, big.mark = ",")` (`r prettyNum(all_word_counts, big.mark = ",")` words) from `r year_num` annual meetings. We obtained PDF files of annual programs from https://www.saa.org/annual-meeting/annual-meeting-archives/program-archives. 

LW: The PDFs for the years 1962-1994 contain images of pages, and after 2005 the PDFs are searchable text. We converted the page images from PDF to PNG files using ImageMagick software, and then used optical character recognition to extract text from the PNG files with the Tesseract package [@Kay2007]. We obtained an acceptable rate of 95-98% word-level OCR accuracy across the PDFs [@Holley2009]. After OCR, we removed punctuation, numbers, and stopwords, such as "the", "an", "a", due to their little or no semantic value for our research question, with the quanteda package [@Benoit2018]. 

LW: We transformed the cleaned text into a document-feature matrix. This is a rectangular, tabular data structure where one row represents one PDF, and one column represents one word, also known as a feature. The cell values in this matrix are the frequencies of each feature for each PDF. We selected the features that occur at least three times across all PDFs for construction of a feature co-occurrence matrix. To create this matrix we calculated how often each feature appears with another. The co-occurrences are measured within a defined range of surrounding words, known as a window, with the target feature in the middle position and context features before and after it. Here we set the range to five, meaning five words before and five words after the target feature, as our framework for calculating the co-occurrence counts. 

LW: We generated the word embeddings on the co-occurrence matrix using the GloVe algorithm from the text2vec package for R [@Pennington2014; @Selivanov2016]. GloVe (Global Vectors) is a unsupervised learning model that creates word vectors to represent semantic regularity and capture relationships between words [@Pennington2014; @Garg2018]. 

<!-- one paragraph of plain and simple description of how Glove computes word vectors from co-oc matrix -->

BM: One of the key practical challenges of studying temporal change in word embeddings is aligning the vectors representing the same words across multiple time periods. This is because the stochastic nature of word embeddings algorithms makes them generate representations that always have different coordinate systems and thus one set of embeddings are are not directly comparable to another set generated on a different corpus (i.e. from a different time period). Previous work generally follows a two-step pattern: first compute word embeddings in each time slice separately, then find a way to align the word embeddings across time slices. @kim2014temporal generated word embeddings for each time period by initializing them with the embeddings from the previous period. @kulkarni2015statistically compute a linear transformation of words between any two time slices by solving a n-dimensional least squares problem of k nearest neighbor words (where n is the embedding dimension). @zhang2016past similarly use a linear transformation between a base and target time slices, and compute the linear transformation using anchor words that does not change meaning between the two time slices (requiring prior knowledge of words that are static). @hamilton2016diachronic solves a n-dimensional Procrustes problem between every two adjacent time slices to perform the alignment. These alignment procedures are complex, and @dubossarsky2017outta show that they suffer from biases and produce spurious results. @dubossarsky2019time show that an alternative method, temporal referencing, avoids these concerns about previous alignment methods using a radically simpler procedure. Temporal referencing combines all time-specific corpora into one corpus, and generates word embeddings on this full corpus. Before generating the embeddings, they first replace each instance of the target word with a time-specific token. For example, the word 'theory' is replaced by the time-specfic token 'theory (1970-1980)' in the meeting abstracts from the years 1970-1980, and 'theory (1980-1990)' in the meeting abstracts from the years 1980-1990, and so on for each time slice. Then a single space is generated that contains a vector for each time-specific token, and the coordinates of the background, or context, words remain the same for each time slice, with no need for alignment. Besides the advantage of avoiding alignment, this temporal referencing method lowers data requirements (because context words are collapsed, and thus shared across time slices) and is suitable for smaller corpora, producing smooth change values without millions of words. A key assumption of this approach, as with the alignment methods, is that the semantics of the context words stay relatively stable over time.

BM: Once the embeddings had been generated, we visualized the word vector values to explore the historical trends of word meanings in archaeology. We first subset the corpus to identify the words that are semantically similar to our target word using the cosine distance measure on pairs of word vectors. These similar words become the context words that map the semantic space that our target word moves through over time. Then we used the t-Distributed Stochastic Neighbor Embedding (t-SNE) method [@maaten2008visualizing] for constructing a two dimensional embedding of the word vectors to reduce the high dimensionality of the word vectors so they can projected on a scatter plot. t-SNE is a visualisation method that retaining the local structure of the data while also revealing some important global structure (such as clusters). This is useful for placing similar words close together on the plot, while maximizing the distance between dissimilar words. While t-SNE is highly flexible, and can often find structure where other dimensionality-reduction algorithms, such as PCA, cannot [@balamurali2016t], it can produce plots that are difficult to interpret, and some elements can be highly senstive to different hyperparameter settings, for example the size of the clusters and inter-cluster distances [@wattenberg2016how]. A key limitation is that because t-SNE is non-linear (compared to PCA which is a linear transformation) and adapts to the underlying data, performing different transformations on different regions of the data, t-SNE plots are not easily interpretable in terms of the axes/units of the original, high-dimensional data.  

<!-- one paragraph of explaining how did we choose these words -->

We created a word list, including theory, inference, mechanism, explanation, hypothesis, to explore how they changes in their meaning or use since 1962. We compare the word embeddings at a interval of ten years.





<!-- There are two major word-level pretrained word embeddings, Word2Vec and Glove according to different methods used for training [@Rong2014; @Pennington2014]. --> 




```{r temporal-referencing-plots, cache.extra=tools::md5sum(here::here("006-testing-word-2-vec-temporal-referencing-by-period.R"))}
source(here::here("code/006-testing-word-2-vec-temporal-referencing-by-period.R"))
```


- basic details (e.g. word and abstract counts)
- word frequency time series
- TR plots

# Discussion

- ...

# Conclusion

- ...

# Acknowledgements

# References 

<!--- use the bib file --> 



